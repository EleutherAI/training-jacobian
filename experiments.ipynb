{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn, struct\n",
    "from flax.training.train_state import TrainState\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from optax import (\n",
    "    softmax_cross_entropy_with_integer_labels as xent\n",
    ")\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    hidden_sizes: tuple[int, ...]\n",
    "    out_features: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for feat in self.hidden_sizes:\n",
    "            scale = feat ** -0.5\n",
    "            bias_init = nn.initializers.normal(stddev=scale)\n",
    "\n",
    "            x = nn.Dense(feat, bias_init=bias_init)(x)\n",
    "            x = nn.gelu(x)\n",
    "\n",
    "        x = nn.Dense(self.out_features, bias_init=bias_init)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class TrainConfig:\n",
    "    batch_size: int = 64\n",
    "    num_epochs: int = 25\n",
    "\n",
    "    opt: str = \"sgd\"\n",
    "\n",
    "\n",
    "def make_apply_full(model, unraveler):\n",
    "    \"\"\"Make an apply function that takes the full parameter vector.\"\"\"\n",
    "    def apply_full(raveled, x):\n",
    "        params = unraveler(raveled)\n",
    "        return model.apply(params, x)\n",
    "    \n",
    "    return apply_full\n",
    "\n",
    "\n",
    "def make_apply_subspace(model, unraveler, params0, basis):\n",
    "    \"\"\"Make an apply function that takes a small parameter vector.\"\"\"\n",
    "    def apply_subspace(small_params, x):\n",
    "        raveled = params0 + basis.T @ small_params\n",
    "        return model.apply(unraveler(raveled), x)\n",
    "    \n",
    "    return apply_subspace\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def compute_loss(params, apply_fn, X, Y):\n",
    "    logits = apply_fn(params['p'], X)\n",
    "    preds = jnp.argmax(logits, axis=-1)\n",
    "\n",
    "    loss = xent(logits, Y).mean()\n",
    "    acc = jnp.mean(preds == Y)\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def train(params, x_train, y_train, x_test, y_test, apply_fn, cfg: TrainConfig):\n",
    "    # Create the batches\n",
    "    X_batched = jnp.reshape(x_train, (-1, cfg.batch_size, 64))\n",
    "    Y_batched = jnp.reshape(y_train, (-1, cfg.batch_size))\n",
    "\n",
    "    # LR schedule\n",
    "    num_steps = cfg.num_epochs * len(x_train) // cfg.batch_size\n",
    "\n",
    "    # Define the optimizer and training state\n",
    "    if cfg.opt == \"adam\":\n",
    "        sched = optax.cosine_decay_schedule(3e-3, num_steps)\n",
    "        tx = optax.adam(learning_rate=sched, eps_root=1e-8)\n",
    "    else:\n",
    "        sched = optax.cosine_decay_schedule(0.1, num_steps)\n",
    "        tx = optax.chain(\n",
    "            optax.sgd(learning_rate=sched, momentum=0.9)\n",
    "        )\n",
    "\n",
    "    state = TrainState.create(apply_fn=apply_fn, params=dict(p=params), tx=tx)\n",
    "\n",
    "    # Forward and backward pass\n",
    "    loss_and_grad = jax.value_and_grad(compute_loss, has_aux=True)\n",
    "\n",
    "    def train_step(state: TrainState, batch):\n",
    "        loss, grads = loss_and_grad(state.params, state.apply_fn, *batch)\n",
    "        return state.apply_gradients(grads=grads), loss\n",
    "\n",
    "    def epoch_step(state: TrainState, epoch) -> tuple[TrainState, tuple[jnp.ndarray, jnp.ndarray]]:\n",
    "        state, (losses, accs) = jax.lax.scan(train_step, state, (X_batched, Y_batched))\n",
    "        return state, (losses.mean(), accs.mean())\n",
    "\n",
    "    state, (train_loss, _) = jax.lax.scan(epoch_step, state, jnp.arange(cfg.num_epochs))\n",
    "    raveled, _ = ravel_pytree(state.params)\n",
    "\n",
    "    # Test loss\n",
    "    logits = state.apply_fn(state.params['p'], x_test)\n",
    "    test_loss = xent(logits, y_test).mean()\n",
    "    return raveled, test_loss#, train_loss # test_loss, train_loss[-1]\n",
    "\n",
    "\n",
    "# grad_fn = jax.value_and_grad(train, has_aux=True)\n",
    "jac_fn = jax.jacfwd(train, has_aux=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load data\n",
    "X, Y = load_digits(return_X_y=True)\n",
    "X = X / 16.0  # Normalize\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=197, random_state=0)\n",
    "spectra = []\n",
    "\n",
    "d_inner = X.shape[1] // 2\n",
    "\n",
    "model = MLP(hidden_sizes=(d_inner,), out_features=10)\n",
    "\n",
    "# Do a single run with seed 0 to get the Jac\n",
    "key = jax.random.key(0)\n",
    "\n",
    "params_0 = model.init(key, X_train)\n",
    "raveled_0, unraveler = ravel_pytree(params_0)\n",
    "apply_fn = make_apply_full(model, unraveler)\n",
    "\n",
    "raveled_f, loss = train(raveled_0, X_train, Y_train, X_test, Y_test, apply_fn, TrainConfig(opt=\"sgd\"))\n",
    "params_f = unraveler(raveled_f)\n",
    "\n",
    "params_0, unravel = ravel_pytree(params_0)\n",
    "jac, losses = jac_fn(raveled_0, X_train, Y_train, X_test, Y_test, apply_fn, TrainConfig(opt=\"sgd\"))\n",
    "jac_u, jac_s, jac_vh = jnp.linalg.svd(jac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "complements = []\n",
    "bulk_y = []\n",
    "responses = []\n",
    "\n",
    "params_f, loss = train(\n",
    "    params_0, X_train, Y_train, X_test, Y_test, apply_fn, TrainConfig(opt=\"sgd\")\n",
    ")\n",
    "\n",
    "num = 2000\n",
    "grid = jnp.logspace(-3, 3, 30)\n",
    "for stim in grid:\n",
    "    perturbed_params, loss = train(\n",
    "        params_0 + stim * jac_vh[num], X_train, Y_train, X_test, Y_test, apply_fn, TrainConfig(opt=\"sgd\")\n",
    "    )\n",
    "    delta = perturbed_params - params_f\n",
    "    res = delta @ jac_u[:, num]\n",
    "    complement = jnp.linalg.norm(delta - res * jac_u[:, num])\n",
    "\n",
    "    complements.append(complement)\n",
    "    bulk_y.append(loss)\n",
    "    responses.append(res)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 4))  # Adjust figure size to be more paper-friendly\n",
    "plt.plot(grid, grid * jac_s[num], label=\"Pred. of Jacobian\", c=\"black\", linestyle=\"--\")\n",
    "plt.plot(grid, jnp.abs(jnp.array(responses)), marker=\"o\", label=\"Proj. on left singular vector\", alpha=0.75)\n",
    "plt.plot(grid, complements, marker=\"o\", label=\"Proj. on complement\", alpha=0.75)\n",
    "plt.plot(grid, bulk_y, marker=\"x\", label=\"Test Loss\", alpha=0.75)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Stimulus Size\")\n",
    "plt.ylabel(\"Response in Final Model\")\n",
    "plt.title(f\"Response to Perturbing Init. Along Bulk Direction\")\n",
    "# plt.title(f\"Response to Perturbing Init. Along Top S.V. ($\\\\sigma_1 = {s[0]:.3f}$)\")\n",
    "plt.legend()\n",
    "\n",
    "# Tight layout to ensure no clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show or save the plot\n",
    "plt.savefig('bulk perturb.pdf', format='pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
